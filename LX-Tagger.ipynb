{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NRjcAqnK2S2Q","executionInfo":{"status":"ok","timestamp":1715005767375,"user_tz":-60,"elapsed":23649,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}},"outputId":"3bca6560-ad3f-4115-b7ab-57088f7a210c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"JV69EXB3vcnv"}},{"cell_type":"code","source":["import json\n","import requests\n","import re\n","import pandas as pd\n","import os"],"metadata":{"id":"011dJuy-7Q2i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Functions"],"metadata":{"id":"VpfDFqIV3NdN"}},{"cell_type":"markdown","source":["This code serves the purpose of tagging Portuguese books with LX-Tagger. In order to do so a working API Key will be needed, its easy to get one, just follow the steps in  https://portulanclarin.net/workbench/lx-tagger/ use your API KEY and Indicate the .txt file of the book you want tagged, some extra-steps may need to be taken depending on the size of the book in question"],"metadata":{"id":"QCPl1PO7Z_3H"}},{"cell_type":"code","source":["class WSException(Exception):\n","    'Webservice Exception'\n","    def __init__(self, errordata):\n","        \"errordata is a dict returned by the webservice with details about the error\"\n","        super().__init__(self)\n","        assert isinstance(errordata, dict)\n","        self.message = errordata[\"message\"]\n","        # see https://json-rpc.readthedocs.io/en/latest/exceptions.html for more info\n","        # about JSON-RPC error codes\n","        if -32099 <= errordata[\"code\"] <= -32000:  # Server Error\n","            if errordata[\"data\"][\"type\"] == \"WebServiceException\":\n","                self.message += f\": {errordata['data']['message']}\"\n","            else:\n","                self.message += f\": {errordata['data']!r}\"\n","    def __str__(self):\n","        return self.message"],"metadata":{"id":"kUWfMG2cx6UP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tag(text, format):\n","    '''\n","    Arguments\n","        text: a string with a maximum of 4000 characters, Portuguese text, with\n","             the input to be processed\n","        format: either 'CINTIL', 'CONLL' or 'JSON'\n","\n","    Returns a string with the output according to specification in\n","       https://portulanclarin.net/workbench/lx-tagger/\n","\n","    Raises a WSException if an error occurs.\n","    '''\n","\n","    request_data = {\n","        'method': 'tag',\n","        'jsonrpc': '2.0',\n","        'id': 0,\n","        'params': {\n","            'text': text,\n","            'format': format,\n","            'key': LXTAGGER_WS_API_KEY,\n","        },\n","    }\n","    request = requests.post(LXTAGGER_WS_API_URL, json=request_data)\n","    response_data = request.json()\n","    if \"error\" in response_data:\n","        raise WSException(response_data[\"error\"])\n","    else:\n","        return response_data[\"result\"]"],"metadata":{"id":"SOidT9FHyEPY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def slice_into_chunks(lines, max_chunk_size=4000):\n","    chunk, chunk_size = [], 0\n","    for lnum, line in enumerate(lines, start=1):\n","        if (chunk_size + len(line)) <= max_chunk_size:\n","            chunk.append(line)\n","            chunk_size += len(line) + 1\n","            # the + 1 above is for the newline character terminating each line\n","        else:\n","            yield \"\\n\".join(chunk)\n","            if len(line) > max_chunk_size:\n","                print(f\"line {lnum} is longer than 4000 characters; truncating\")\n","                line = line[:4000]\n","            chunk, chunk_size = [line], len(line) + 1\n","    if chunk:\n","        yield \"\\n\".join(chunk)"],"metadata":{"id":"yhYOEcX5HEzJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here Introduce you API Key"],"metadata":{"id":"uh0WDFura4N5"}},{"cell_type":"code","source":["LXTAGGER_WS_API_KEY = '8364bfed4ce5ab699e6f23279e5cdad6'\n","LXTAGGER_WS_API_URL = 'https://portulanclarin.net/workbench/lx-tagger/api/'"],"metadata":{"id":"mIWExnNj3u49"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here Indicate the Path to the file for tagging"],"metadata":{"id":"PkynrqmbbCog"}},{"cell_type":"code","source":["path='/content/drive/MyDrive/SNA Pipeline (Tese)/Livros (1)/O Crime do Padre Amaro.txt'"],"metadata":{"id":"2VQQXPZC9Ui4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tag in chunks"],"metadata":{"id":"PhpbAdkTkBm3"}},{"cell_type":"markdown","source":["Here excel_path needs to be indicated"],"metadata":{"id":"GzIeXXf8bQqu"}},{"cell_type":"code","source":["\n","def Import_to_Tag(path, start_chunk=1):\n","    with open(path, 'r', encoding='utf-8') as file:\n","        lines = file.readlines()\n","\n","    sample_text_lines = [line.strip() for line in lines if line.strip()]\n","    chunks = list(slice_into_chunks(sample_text_lines))\n","    annotated_text = []\n","\n","    final_chunk_processed = start_chunk  # Initialize with start_chunk in case no chunks are processed\n","\n","    for cnum, chunk in enumerate(chunks[start_chunk-1:], start=start_chunk):\n","        try:\n","            annotated_text.extend(tag(chunk, format=\"JSON\"))\n","            final_chunk_processed = cnum  # Update final_chunk_processed after each successful chunk processing\n","            print(\".\", end=\"\", flush=True)  # Progress feedback\n","        except Exception as exc:\n","            chunk_preview = ' '.join(chunk.split()[:10]) + \"...\" if len(chunk.split()) > 10 else chunk\n","            print(f\"\\nError: annotation of chunk {cnum} failed ({exc}); chunk contents:\\n{chunk_preview}\\n\")\n","            break\n","\n","    # Extracting book name from the path for the Excel filename\n","    book_name_match = re.search(r'/([^/]+)\\.txt', path)\n","    book_name = book_name_match.group(1) if book_name_match else \"output\"\n","\n","    # Adding chunk range to the book name\n","    book_name_with_chunks = f\"{book_name} ({start_chunk}-{final_chunk_processed})\"\n","\n","    # Convert the annotated text to a DataFrame and save it to an Excel file\n","    df = pd.DataFrame(annotated_text)\n","    excel_path = f'/content/drive/MyDrive/SNA Pipeline (Tese)/POS Books (1)/POS {book_name_with_chunks}.xlsx'\n","    df.to_excel(excel_path, index=False)\n","    print(f'\\nYour book \"{book_name_with_chunks}\" was tagged :))')\n","\n"],"metadata":{"id":"wszFSVt3Ppc6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Start with 1 and indicate the next starting chunk in case multiple runs are needed"],"metadata":{"id":"Vayaiwpdc4lr"}},{"cell_type":"code","source":["Import_to_Tag(path,200)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MIkKk3UiRCQN","executionInfo":{"status":"ok","timestamp":1715006104322,"user_tz":-60,"elapsed":30433,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}},"outputId":"137ae0b2-3c45-4857-fb86-0ea4e2a91f67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".....................\n","Your book \"O Crime do Padre Amaro (200-220)\" was tagged :))\n"]}]},{"cell_type":"markdown","source":["## Join the Chunks"],"metadata":{"id":"kNOOpcN-kT5E"}},{"cell_type":"markdown","source":["In case the book in question was too large for a single file you run this code in order to join all anotated chunks\n","Paths need to be indicated"],"metadata":{"id":"rgdXCr2fbaD1"}},{"cell_type":"code","source":["def consolidate_book_chunks(path):\n","\n","    dir_path = \"/content/drive/MyDrive/SNA Pipeline (Tese)/POS Books (1)\"\n","\n","    # Extract the base book name from the path, without the .txt extension\n","    book_name_match = re.search(r'/([^/]+)\\.txt$', path)\n","    if not book_name_match:\n","        print(\"Invalid book path.\")\n","        return\n","    book_name = book_name_match.group(1)\n","    print(book_name)\n","\n","    # Pattern to match files starting with \"POS\", followed by the book name, and a chunk range in parentheses\n","    pattern = re.compile(rf'^POS {re.escape(book_name)} \\((\\d+)-(\\d+)\\)\\.xlsx$')\n","    print(pattern)\n","    # List all files in the directory\n","    try:\n","        files = os.listdir(dir_path)\n","        print(files)\n","    except FileNotFoundError:\n","        print(f\"Directory not found: {dir_path}\")\n","        return\n","\n","    # Filter and sort files based on the chunk range\n","    book_files = [file for file in files if pattern.match(file)]\n","    book_files.sort(key=lambda x: int(pattern.match(x).group(1)))\n","\n","    print(book_files)\n","    if not book_files:\n","        print(f\"No chunked files found for the book: {book_name}\")\n","        return\n","\n","    master_df = pd.DataFrame()\n","\n","    # Process and append data from each file\n","    for file in book_files:\n","        df = pd.read_excel(os.path.join(dir_path, file))\n","        master_df = pd.concat([master_df, df], ignore_index=True)\n","\n","    # Get the full range from the first and last file\n","    first_chunk = pattern.match(book_files[0]).group(1)\n","    last_chunk = pattern.match(book_files[-1]).group(2)\n","    consolidated_filename = f\"POS {book_name} ({first_chunk}-{last_chunk}).xlsx\"\n","    consolidated_path = os.path.join( \"/content/drive/MyDrive/SNA Pipeline (Tese)/POS Books (1)/complete (1)\", consolidated_filename)\n","\n","    # Save the consolidated DataFrame\n","    master_df.to_excel(consolidated_path, index=False)\n","    print(f'Consolidated book saved as \"{consolidated_filename}\"')\n","\n","\n","\n"],"metadata":{"id":"fJljKOFtMxVT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def consolidate_book_chunks(path):\n","    dir_path = \"/content/drive/MyDrive/SNA Pipeline/POS Books\"\n","\n","    # Extract the base book name from the path, without the .txt extension\n","    book_name_match = re.search(r'/([^/]+)\\.txt$', path)\n","    if not book_name_match:\n","        print(\"Invalid book path.\")\n","        return\n","    book_name = book_name_match.group(1)\n","    print(\"Book name extracted:\", book_name)\n","\n","    # Adjust the regex pattern to ensure it matches any content between the book name and the chunk range\n","    # The change here is to make sure we capture any text (including \"integral\" or similar) following the book name right up to the chunk numbers\n","    pattern = re.compile(rf'^POS {re.escape(book_name)} \\(?(\\d+)-(\\d+)\\)?\\.xlsx$')\n","\n","    try:\n","        files = os.listdir(dir_path)\n","        print(\"Files found:\", files)\n","    except FileNotFoundError:\n","        print(f\"Directory not found: {dir_path}\")\n","        return\n","\n","    # Filter and sort files based on the chunk range\n","    book_files = [file for file in files if pattern.match(file)]\n","    book_files.sort(key=lambda x: int(pattern.match(x).group(1)))\n","\n","    if not book_files:\n","        print(f\"No chunked files found for the book: {book_name}\")\n","        return\n","\n","    master_df = pd.DataFrame()\n","\n","    # Process and append data from each file\n","    for file in book_files:\n","        df = pd.read_excel(os.path.join(dir_path, file))\n","        master_df = pd.concat([master_df, df], ignore_index=True)\n","\n","    # Derive consolidated filename from the range of chunks\n","    first_chunk = pattern.match(book_files[0]).group(1)\n","    last_chunk = pattern.match(book_files[-1]).group(2)\n","    consolidated_filename = f\"POS {book_name} ({first_chunk}-{last_chunk}).xlsx\"\n","    consolidated_path = os.path.join(dir_path, \"complete\", consolidated_filename)\n","\n","    # Save the consolidated DataFrame\n","    master_df.to_excel(consolidated_path, index=False)\n","    print(f'Consolidated book saved as \"{consolidated_filename}\"')\n"],"metadata":{"id":"54WIQi0gT6qy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["consolidate_book_chunks(path)"],"metadata":{"id":"Dk1as8_nSx7H","executionInfo":{"status":"ok","timestamp":1715006369139,"user_tz":-60,"elapsed":2820,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ce8b61fd-64a0-4495-e4d9-01ca298e9971"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["O Crime do Padre Amaro\n","re.compile('^POS O\\\\ Crime\\\\ do\\\\ Padre\\\\ Amaro \\\\((\\\\d+)-(\\\\d+)\\\\)\\\\.xlsx$')\n","['complete (1)', 'full names (1)', 'pos antigos', 'complete antigo', 'POS O Crime do Padre Amaro (1-199).xlsx', 'POS O Crime do Padre Amaro (200-220).xlsx']\n","['POS O Crime do Padre Amaro (1-199).xlsx', 'POS O Crime do Padre Amaro (200-220).xlsx']\n","Consolidated book saved as \"POS O Crime do Padre Amaro (1-220).xlsx\"\n"]}]}]}