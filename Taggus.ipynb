{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"NRjcAqnK2S2Q","colab":{"base_uri":"https://localhost:8080/","height":311},"executionInfo":{"status":"error","timestamp":1722686056665,"user_tz":-60,"elapsed":4994,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}},"outputId":"6a7aabad-356a-471b-db32-b7488208e90e"},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"JV69EXB3vcnv"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdQCYKcMvk4I","executionInfo":{"status":"aborted","timestamp":1722686056666,"user_tz":-60,"elapsed":18,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["!pip install spacy\n","!python -m spacy download pt_core_news_md\n","!pip install seaborn\n","!pip install fuzzywuzzy\n","!pip install nameparser"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"011dJuy-7Q2i","executionInfo":{"status":"aborted","timestamp":1722686056666,"user_tz":-60,"elapsed":18,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["import ast\n","import chardet\n","import json\n","import matplotlib.pyplot as plt\n","import networkx as nx\n","import os\n","import pandas as pd\n","import re\n","import requests\n","import seaborn as sns\n","import spacy\n","from ast import literal_eval\n","from collections import Counter, defaultdict\n","from fuzzywuzzy import process\n","from itertools import combinations, product\n","from nameparser import HumanName\n","from networkx.algorithms import community\n","\n","\n","#Load NLP package\n","\n","nlp = spacy.load('pt_core_news_md')"]},{"cell_type":"markdown","metadata":{"id":"lb8a9CpTsp7G"},"source":["#Load data"]},{"cell_type":"markdown","source":["Load Book in analysis and perform NLP"],"metadata":{"id":"r8z7PEidkcnq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"561calqAuP_P","executionInfo":{"status":"aborted","timestamp":1722686056667,"user_tz":-60,"elapsed":17,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["#Tag book with spacy\n","\n","doc= open('/content/drive/MyDrive/SNA Pipeline (Tese)/Livros (1)/Amor de Perdição.txt', 'r', encoding='utf-8').read()\n","\n","#NLP max lenght might need to be increased depending in lenght of the book (might affect performance)\n","nlp.max_length= 1500000\n","\n","book = nlp(doc)\n"]},{"cell_type":"markdown","source":["Retrieve Tags done by LX-Tagger"],"metadata":{"id":"KqA95Zl2knyq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"J-L0ARdsuP_X","executionInfo":{"status":"aborted","timestamp":1722686056667,"user_tz":-60,"elapsed":17,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["#Load book tagged by LX-Tagger\n","\n","ltags = pd.read_excel('/content/drive/MyDrive/SNA Pipeline (Tese)/POS Books (1)/complete (1)/POS Amor de Perdição (1-72).xlsx')\n"]},{"cell_type":"markdown","metadata":{"id":"1jPB4pIyATT3"},"source":["#Name Retrieval and Information extraction"]},{"cell_type":"markdown","source":["##Identify Character Names"],"metadata":{"id":"QDQCVxiyljRE"}},{"cell_type":"markdown","metadata":{"id":"k9ztc8h_LZ24"},"source":["Create a list of every candidate for sequences of names in LX-tagger and count ocurrences for future reference as well as preceding tokens for context"]},{"cell_type":"code","source":["# List to store sequences\n","sequences = []\n","\n","# Dictionary to store sequence counts\n","sequence_counter = {}\n","\n","# Set to keep track of unique sequences\n","printed_sequences = set()\n","\n","# Dictionary to store preceding tokens for each sequence\n","preceding_tokens_dict = {}\n","\n","# Variable to store the last unfinished sequence\n","last_sequence = []\n","\n","for _, row in ltags.iterrows():\n","    for cell in row.dropna():\n","        try:\n","            sentence = ast.literal_eval(cell)  # Convert string to list of tokens\n","        except ValueError as e:\n","            print(f\"Error converting cell content to list: {e}\")\n","            continue\n","\n","        i = 0  # Initialize counter i\n","        while i < len(sentence):\n","            token = sentence[i]\n","            # Check if token is PNM or STT and has a form\n","            if token['pos'] == 'PNM' and token['form']:\n","                pnm_sequence = [token['form']]  # Start a new sequence with the current PNM form\n","                # Look back for the two preceding tokens\n","                j = max(i - 2, 0)\n","                preceding_tokens = [sentence[k]['form'] for k in range(j, i)]\n","                i += 1  # Move to the next token\n","            elif token['pos'] == 'STT' and i + 1 < len(sentence) and sentence[i + 1]['pos'] == 'PNM':\n","                pnm_sequence = [token['form'], sentence[i + 1]['form']]  # Start a new sequence with STT and PNM\n","                # Look back for the two preceding tokens\n","                j = max(i - 2, 0)\n","                preceding_tokens = [sentence[k]['form'] for k in range(j, i)]\n","                i += 2  # Skip the next token as it's already included\n","            else:\n","                i += 1  # Move to the next token if current is not a valid start\n","                continue  # Continue to the next iteration of the loop\n","\n","            # Look ahead for PREP (containing \"de\"), DA, PNM patterns\n","            j = i  # Start looking ahead from the current position\n","            while j < len(sentence):\n","                if sentence[j]['pos'] == 'PREP' and \"de\" in sentence[j]['form'].lower():\n","                    if j + 1 < len(sentence) and sentence[j + 1]['pos'] == 'PNM':\n","                        pnm_sequence += [sentence[j]['form'], sentence[j + 1]['form']]  # Add PREP and PNM\n","                        j += 2  # Move past the added tokens\n","                    elif j + 2 < len(sentence) and sentence[j + 1]['pos'] == 'DA' and sentence[j + 2]['pos'] == 'PNM':\n","                        pnm_sequence += [sentence[j]['form'], sentence[j + 1]['form'], sentence[j + 2]['form']]  # Add PREP, DA, PNM\n","                        j += 3  # Move past the added tokens\n","                    else:\n","                        break  # Exit loop if next token is not as expected\n","                elif sentence[j]['pos'] == 'PNM' and sentence[j]['form']:\n","                    pnm_sequence.append(sentence[j]['form'])  # Add another PNM\n","                    j += 1  # Move to the next token\n","                else:\n","                    break  # Exit loop if next token is not PNM or the correct sequence\n","\n","            # Check if a valid sequence was found and it's not just a single STT token\n","            if len(pnm_sequence) > 1 or (len(pnm_sequence) == 1 and pnm_sequence[0] != 'STT'):\n","                # Check if the last token processed in the current row is the last token before changing to the next row\n","                if i == len(sentence):\n","                    last_sequence = pnm_sequence  # Store the unfinished sequence for the next row\n","                else:\n","                    # If there is an unfinished sequence from the previous row, add it to the current sequence\n","                    if last_sequence:\n","                        pnm_sequence = last_sequence + pnm_sequence\n","                        last_sequence = []  # Reset the last_sequence variable\n","                    sequence_str = \" \".join(pnm_sequence)\n","                    if sequence_str not in printed_sequences:\n","                        sequences.append(sequence_str)\n","                        printed_sequences.add(sequence_str)\n","                    sequence_counter[sequence_str] = sequence_counter.get(sequence_str, 0) + 1  # Increment count or initialize if new\n","                    # Store preceding tokens for the current sequence\n","                    preceding_tokens_dict.setdefault(sequence_str, []).append(preceding_tokens)\n","            i = j  # Update the outer loop counter to resume from where the inner loop left off\n","\n","# Remove all occurrences of \"…\" from sequences list\n","sequences = [sequence.replace(\"…\", \"\") for sequence in sequences]\n","\n","# Sort sequence_counter in descending order based on counts\n","sorted_sequence_counter = {k: v for k, v in sorted(sequence_counter.items(), key=lambda item: item[1], reverse=True)}\n"],"metadata":{"id":"2jC3V10DjpnA","executionInfo":{"status":"aborted","timestamp":1722686056667,"user_tz":-60,"elapsed":16,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sorted_sequence_counter"],"metadata":{"id":"VibcPT8OwhA6","executionInfo":{"status":"aborted","timestamp":1722686056667,"user_tz":-60,"elapsed":16,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y2od-lTRMTi3"},"source":["Identify all sequences that are confirmed to be names by proceding a oficial title, add them to a final list and remove them from current working list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhgVeFUIe65r","executionInfo":{"status":"aborted","timestamp":1722686056668,"user_tz":-60,"elapsed":16,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["# List of portuguese Titles\n","titles = {\"dr\", \"d.r\", \"dr.\", \"dra.\", \"dra\", \"senhor\", \"senhora\", \"d.ra\", \"sr\", \"sr.\", \"sra\", \"sra.\", \"prof\", \"prof.\", \"profa\", \"eng.\", \"eng\", \"d\", \"d.\", \"v.\", \"ex.\", \"ex.a\", \"s.ª\", \"ex.ª\", \"dom\", \"d.ª\", \"da.\", \"il.mo\", \"ilmo.\", \"ilmo\", \"il.ma\", \"ilma.\", \"ilma\", \"rev\"}\n","\n","matching_sequences = set()  # To store sequences excluding the initial title or STT tag\n","\n","# Part 1: Handling Titles and STT Tags\n","# Create a copy of the original sequences list to avoid modifying it while iterating\n","sequences_copy = sequences.copy()\n","\n","for sequence in sequences_copy:\n","    words = sequence.split()\n","    if not words:\n","        continue  # Skip empty sequences\n","\n","    first_word = words[0].lower()  # Lowercase for case-insensitive comparison\n","    is_title = first_word in titles\n","    is_stt = first_word == \"[stt]\"\n","\n","    if is_title or is_stt:\n","        title_or_stt = words[0]  # The original title or STT tag\n","        rest_of_sequence = words[1:]  # Sequence excluding the title or STT tag\n","        # Check if the next word (if exists) also belongs to titles or is an \"STT\" tag\n","        if rest_of_sequence and (rest_of_sequence[0].lower() in titles or rest_of_sequence[0].lower() == \"[stt]\"):\n","            print(f\"Excluded: {title_or_stt}, {rest_of_sequence[0]}\")  # For visibility\n","            # Exclude the first two words if the second word also belongs to titles or is an \"STT\" tag\n","            rest_of_sequence = rest_of_sequence[1:]  # Exclude the second word\n","        print(f\"Excluded: {title_or_stt}, {' '.join(rest_of_sequence)}\")  # For visibility\n","        if rest_of_sequence:\n","            matching_sequences.add(\" \".join(rest_of_sequence))  # Add to matching_sequences if not empty\n","            sequences.remove(sequence)  # Remove the sequence from the original list\n","\n","# Update 'sequences' to exclude processed sequences\n","sequences = [seq for seq in sequences if seq not in matching_sequences]\n"]},{"cell_type":"code","source":["# List of phrases indicating a person in preceding tokens\n","indicators = [\n","    \"amigo de\", \"conhecido de\", \"colega de\", \"cônjuge de\", \"disse\", \"dizia\", \"diz\", \"inquiria\", \"debateu\", \"dialogou\",\n","    \"discutiu\", \"disse-me\", \"gritava\", \"gritou\", \"perguntou\", \"suspirou\", \"suspirava\", \"reclamou\", \"reclamava\",\n","    \"esposa de\", \"viúva\", \"viúvo\", \"familiar de\", \"falou\", \"filha de\", \"filho de\", \"irmã de\", \"irmão de\",\n","    \"marido de\", \"mãe de\", \"namorado de\",\"tio\",\"tia\", \"neto de\", \"opinião de\", \"palavras de\", \"pai de\", \"parente de\",\n","    \"perguntar a\", \"perguntou a\", \"prima de\", \"primo de\", \"respondeu\", \"retorquiu\", \"sussurrou\", \"vizinho de\",\n","    \"os filhos de\", \"as filhas de\", \"tenente\", \"soldado\", \"alferes\", \"major\", \"general\", \"comandante\", \"capitão\",\n","    \"capitã\", \"barão\", \"baronesa\", \"detetive\", \"inspetor\", \"professor\", \"professora\", \"doutor\", \"doutora\",\n","    \"mestre\", \"mestra\", \"padre\", \"madre\", \"reverendo\", \"cónego\",\"bispo\", \"chamava-se\", \"se chamava\", \"se chamar\",\"a srª\",\"o srº\"\n","]\n","\n","# Cross-reference sequences in cleaned_sequences with the preceding_tokens_dict\n","for sequence in sequences:\n","    if sequence in preceding_tokens_dict:\n","        preceding_tokens = preceding_tokens_dict[sequence]\n","        for tokens in preceding_tokens:\n","            # Join tokens with space to form the preceding text\n","            preceding_text = \" \".join(tokens)\n","\n","            # Split the preceding text into individual tokens\n","            preceding_token_list = preceding_text.split()\n","\n","            # Token matching\n","            token_matching_phrases = []\n","            for phrase in indicators:\n","                phrase_tokens = phrase.split()\n","                for i in range(len(preceding_token_list) - len(phrase_tokens) + 1):\n","                    if preceding_token_list[i:i + len(phrase_tokens)] == phrase_tokens:\n","                        token_matching_phrases.append(phrase)\n","                        break\n","\n","            if token_matching_phrases:\n","                # Add the sequence to 'matching_sequences' set\n","                matching_sequences.add(sequence)\n","                try:\n","                    sequences.remove(sequence)\n","                except ValueError:\n","                    pass  # Sequence not found in 'sequences', ignore\n","\n","                # Print the matching sequence and preceding tokens\n","                print(f\"Sequence: {sequence}\")\n","                print(f\"Matching Preceding Tokens: {preceding_text}\")\n","                print(f\"Matching Phrases: {', '.join(token_matching_phrases)}\")\n","                break  # Move to the next sequence\n","\n","# Print the remaining sequences in 'sequences'\n","print(\"\\nRemaining Sequences:\")\n","for sequence in sequences:\n","    print(sequence)"],"metadata":{"id":"9j2W_rNHJeeJ","executionInfo":{"status":"aborted","timestamp":1722686056668,"user_tz":-60,"elapsed":16,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kTdHyVDCZUw2"},"source":["Keep treating remaining working list"]},{"cell_type":"markdown","metadata":{"id":"OlYScEz7L8-O"},"source":["Re-tag sequences using spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cJcZlMAeudKA","executionInfo":{"status":"aborted","timestamp":1722686056668,"user_tz":-60,"elapsed":15,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["# Create an empty list to store tagged sequences\n","tagged_sequences = []\n","\n","# Iterate over each phrase in sequences\n","for phrase in sequences:\n","    # Split the phrase into words\n","    words = phrase.split()\n","\n","    # Create an empty list to store tagged words\n","    tagged_words = []\n","\n","    # Process each word individually\n","    for word in words:\n","        # Apply spaCy processing to each word\n","        word_lis = nlp(word)\n","\n","        # Store the word and its tag in a tuple\n","        tagged_word = [(token.text, token.tag_) for token in word_lis]\n","\n","        # Append the tagged word to the list of tagged words\n","        tagged_words.extend(tagged_word)\n","\n","    # Append the tagged words to the list of tagged sequences\n","    tagged_sequences.append(tagged_words)\n","\n","# Now, tagged_sequences contains the sequences of words with their corresponding tags\n","# Iterate over each tagged sequence\n","for tagged_sequence in tagged_sequences:\n","    # Print each word along with its tag\n","    for word, tag in tagged_sequence:\n","        print(f\"Word: {word}, Tag: {tag}\")\n","\n","    # Print a newline to separate each sequence\n","    print()\n"]},{"cell_type":"markdown","source":["Exclude starting words in sequences that were mistankenly tagged as Proper Names Until one is found or remove sequence"],"metadata":{"id":"J8CI00NOnQ4r"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hu3LiIjhudKD","executionInfo":{"status":"aborted","timestamp":1722686056668,"user_tz":-60,"elapsed":15,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["\n","# Create empty lists to store filtered sequences and removed words for each sequence\n","filtered_sequences = []\n","removed_words_per_sequence = []\n","\n","# Iterate over each tagged sequence\n","for tagged_sequence in tagged_sequences:\n","    # Iterate through each word and its tag in the sequence\n","    for idx, (word, tag) in enumerate(tagged_sequence):\n","        # If the word is tagged as a proper noun (PROPN)\n","        if tag == 'PROPN':\n","            # Keep the sequence starting from this word\n","            filtered_sequence = tagged_sequence[idx:]\n","            filtered_sequences.append(filtered_sequence)\n","            # Store the removed words before the proper noun\n","            removed_words = [w for w, _ in tagged_sequence[:idx]]\n","            removed_words_per_sequence.append(removed_words)\n","            break\n","    else:\n","        # If no proper noun is found in the sequence, add the whole sequence to removed_sequences\n","        removed_words_per_sequence.append([w for w, _ in tagged_sequence])\n","\n","# Now, filtered_sequences contains the sequences starting from the first proper noun,\n","# and removed_words_per_sequence contains the removed words for each sequences\n","removed_words_per_sequence"]},{"cell_type":"markdown","source":["Get remaining sequences in string format"],"metadata":{"id":"f4E1iOiqpoOe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Heizawb4udKF","executionInfo":{"status":"aborted","timestamp":1722686056669,"user_tz":-60,"elapsed":13,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["# Extract words from filtered sequences without tags\n","filtered_sequences_without_tags = [[word for word, _ in sequence] for sequence in filtered_sequences]\n","\n","# Print the filtered sequences without tags\n","for sequence in filtered_sequences_without_tags:\n","    print(' '.join(sequence))\n","\n","# Make filtered_sequences_without_tags the main list to work from\n","sequences_to_work_from = filtered_sequences_without_tags\n","\n","# Convert each inner list into a string\n","names_list = [' '.join(sequence) for sequence in sequences_to_work_from]\n","\n","# Print the sequences as strings\n","for sequence in names_list:\n","    print(sequence)\n"]},{"cell_type":"markdown","source":["Import Cities and countries names (more countries can be added)"],"metadata":{"id":"oIhAd07Rpv5o"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9uk5qrYwudKG","executionInfo":{"status":"aborted","timestamp":1722686056669,"user_tz":-60,"elapsed":12,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["cities = pd.read_excel('/content/drive/MyDrive/SNA Pipeline/Geo Places PT/worldcities.xlsx')\n","cities = cities[cities['country'].isin(['Portugal', 'Spain', 'Brasil','France'])]"]},{"cell_type":"markdown","source":["Remove entries that are exclusevly city names (doenst affect names that have a name of a city)"],"metadata":{"id":"BpBMRFR4p6Gr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dvGMNJ9EudKH","executionInfo":{"status":"aborted","timestamp":1722686056669,"user_tz":-60,"elapsed":12,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["filtered_sequences = []  # To store sequences after applying both filters\n","removed_words_combined = []  # To store words removed due to being city names or before the first PROPN\n","\n","# Iterate over each tagged sequence\n","for tagged_sequence in tagged_sequences:\n","    sequence_modified = []  # Temporarily stores the sequence after modification\n","    removed_words_temp = []  # Temporarily stores removed words for this sequence\n","    proper_noun_found = False  # Flag to indicate the first PROPN not a city is found\n","\n","    for idx, (word, tag) in enumerate(tagged_sequence):\n","        # Skip proper nouns that are city names before finding the first significant PROPN\n","        if tag == 'PROPN' and not proper_noun_found:\n","            if word in list(cities['city']):\n","                removed_words_temp.append(word)\n","                continue\n","            else:\n","                proper_noun_found = True  # Mark that we've found our starting PROPN\n","\n","        # After finding the first significant PROPN, add all words to the sequence\n","        if proper_noun_found:\n","            sequence_modified.append((word, tag))\n","        else:\n","            # If no proper noun has been found yet, add words to removed list\n","            removed_words_temp.append(word)\n","\n","    # If a proper noun that is not a city name has been found, store the modified sequence\n","    if proper_noun_found:\n","        filtered_sequences.append(sequence_modified)\n","    # Always store the removed words\n","    removed_words_combined.append(removed_words_temp)\n","removed_words_combined"]},{"cell_type":"markdown","source":["Get current working list"],"metadata":{"id":"9PoVSqWiqTTL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IcZYnAB-udKJ","executionInfo":{"status":"aborted","timestamp":1722686056670,"user_tz":-60,"elapsed":13,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["# Extract words from filtered sequences without tags\n","filtered_sequences_without_tags = [[word for word, _ in sequence] for sequence in filtered_sequences if sequence]\n","\n","# Convert each inner list into a string, ensuring it only works with non-empty sequences\n","names_list = [' '.join(sequence) for sequence in filtered_sequences_without_tags]\n","\n","# Print the sequences as strings, skipping empty ones\n","for sequence in names_list:\n","    if sequence:  # This checks if the sequence string is not empty\n","        print(sequence)"]},{"cell_type":"markdown","source":["Proces to remove words that have been incorrectly agregated with names by finding lower case versions in text"],"metadata":{"id":"-frLi8KXqZEV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmGQIQ-iudKL","executionInfo":{"status":"aborted","timestamp":1722686056670,"user_tz":-60,"elapsed":12,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["def clean_sequences(sequences, document_text, spacy_model):\n","    # Process the document text with the spacy model\n","    doc = spacy_model(document_text)\n","\n","    # Set to store lowercase words from the document\n","    lowercase_words = {token.text.lower() for token in doc if token.text.islower()}\n","\n","    cleaned_sequences = []  # List to store cleaned sequences\n","\n","    # Iterate through each sequence in the list\n","    for sequence in sequences:\n","        # Split the sequence into words\n","        words = sequence.split()\n","\n","        # Check if the first word in the sequence, when converted to lowercase, exists in the document\n","        if words[0].lower() in lowercase_words:\n","            # Remove the first word from the sequence if it exists in lowercase_words\n","            cleaned_words = words[1:]\n","            cleaned_sequence = \" \".join(cleaned_words)\n","        else:\n","            cleaned_sequence = sequence\n","\n","        # Check if the cleaned sequence is not empty after stripping whitespace\n","        if cleaned_sequence.strip():\n","            cleaned_sequences.append(cleaned_sequence)\n","\n","    return cleaned_sequences\n","\n","    # Call the modified function\n","cleaned_sequences = clean_sequences(names_list, book, nlp)\n","\n","# Print the cleaned sequences\n","for sequence in cleaned_sequences:\n","    print(sequence)"]},{"cell_type":"markdown","source":["Lastly verify sequences by crossing them with a data base of Portuguese names"],"metadata":{"id":"66NwG38trH5T"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"X9hI4qtauSzg","executionInfo":{"status":"aborted","timestamp":1722686056670,"user_tz":-60,"elapsed":11,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["\n","# Define file paths\n","file_path_txt = '/content/drive/MyDrive/SNA Pipeline/Names/names_final.txt'\n","file_paths_csv = [\n","    '/content/drive/MyDrive/SNA Pipeline/Names/nomes-registados-2016.csv',\n","    '/content/drive/MyDrive/SNA Pipeline/Names/nomes-registados-2015.csv'\n","]\n","\n","# Process the text file\n","unique_names = set()\n","with open(file_path_txt, 'r') as file:\n","    unique_names.update(name.strip().capitalize() for name in file if name.strip())\n","\n","# Function to read a CSV file and return the first column as a set\n","def read_csv_first_column(file_path):\n","    df = pd.read_csv(file_path, header=None, encoding='utf-8', usecols=[0])\n","    return set(df.iloc[:, 0].str.capitalize().str.strip())\n","\n","# Process CSV files and update the unique names set\n","for file_path in file_paths_csv:\n","    unique_names.update(read_csv_first_column(file_path))\n","\n","# Convert the set to a sorted list\n","sorted_unique_names = sorted(unique_names)\n"]},{"cell_type":"markdown","metadata":{"id":"ldPCBS5p6Lgp"},"source":["Join treated Names with the Names aquired in the first step, create a list of the sequences that were not confirmed as names for possible future work"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48Rtubm5Xr0A","executionInfo":{"status":"aborted","timestamp":1722686056670,"user_tz":-60,"elapsed":11,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["non_matching_sequences = []\n","\n","for sequence in cleaned_sequences:\n","    if sequence:  # Ensure the sequence is not empty\n","        first_word = sequence.split()[0]  # Extract the first word\n","        # Check if the first word matches any name in unique_names\n","        if first_word in unique_names:\n","            matching_sequences.add(sequence)\n","        else:\n","            non_matching_sequences.append(sequence)\n","\n","# Print the matching sequences\n","print(\"Matching Sequences:\")\n","for matched_sequence in matching_sequences:\n","    print(matched_sequence)\n","\n","# Print the non-matching sequences\n","print(\"\\nNon-Matching Sequences:\")\n","for non_matched_sequence in non_matching_sequences:\n","    print(non_matched_sequence)\n"]},{"cell_type":"markdown","source":["Clean titles and religious evocations"],"metadata":{"id":"O96RNXh-5b0p"}},{"cell_type":"code","source":["# Define the set of titles\n","titles = {\n","    \"dr\", \"d.r\", \"dr.\", \"dra.\", \"dra\", \"senhor\", \"senhora\", \"d.ra\", \"s.\", \"sr\", \"sr.\", \"sra\", \"sra.\",\n","    \"prof\", \"prof.\", \"profa\", \"eng.\", \"eng\", \"d\", \"d.\", \"v.\", \"ex.\", \"ex.a\", \"s.ª\", \"ex.ª\", \"dom\",\n","    \"d.ª\", \"da.\", \"il.mo\", \"ilmo.\", \"ilmo\", \"il.ma\", \"ilma.\", \"ilma\", \"rev\", \"vossa senhoria\", \"vossa excelência\",\"sua excelência\", \"jesus\",\"diabo\",\"satanás\",\n","    \"deus\", \"jesus cristo\",\"nosso\", \"nossa\", \"ó\"}\n","\n","# Define the set of titles in lowercase\n","titles_lower = {title.lower() for title in titles}\n","\n","# List to hold sequences that are not titles\n","filtered_sequences = set()\n","\n","# Convert matching_sequences to a set to remove duplicates\n","matching_sequences_set = set(matching_sequences)\n","\n","# Iterate through each matching sequence\n","for sequence in matching_sequences_set:\n","    # Check if the entire sequence in lowercase is not in the set of lowercase titles\n","    if sequence.lower() not in titles_lower:\n","        filtered_sequences.add(sequence)\n","\n","# Print the filtered sequences\n","print(\"Filtered Sequences:\")\n","for filtered_sequence in filtered_sequences:\n","    print(filtered_sequence)\n","\n","# Assign the filtered set back to matching_sequences\n","matching_sequences = filtered_sequences"],"metadata":{"id":"9pMoo4FI5TZx","executionInfo":{"status":"aborted","timestamp":1722686056670,"user_tz":-60,"elapsed":11,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H41J8MLAkFhS"},"source":["##Atribute Gender to name sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQqBeUaD2X8C","executionInfo":{"status":"aborted","timestamp":1722686056674,"user_tz":-60,"elapsed":8727,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["non_matching_sequences = []\n","\n","for sequence in cleaned_sequences:\n","    if sequence:  # Ensure the sequence is not empty\n","        first_word = sequence.split()[0]  # Extract the first word\n","        # Check if the first word matches any name in unique_names\n","        if first_word in unique_names:\n","            matching_sequences.add(sequence)\n","        else:\n","            non_matching_sequences.append(sequence)\n","\n","# Print the matching sequences\n","print(\"Matching Sequences:\")\n","for matched_sequence in matching_sequences:\n","    print(matched_sequence)\n","\n","# Print the non-matching sequences\n","print(\"\\nNon-Matching Sequences:\")\n","for non_matched_sequence in non_matching_sequences:\n","    print(non_matched_sequence)\n"]},{"cell_type":"markdown","metadata":{"id":"3yvSeWvynGdb"},"source":["Prepare Gender database\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8DyQEkynF4C","executionInfo":{"status":"aborted","timestamp":1722686056675,"user_tz":-60,"elapsed":8727,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["# Function to read a CSV file and return a dictionary mapping names to genders\n","def read_csv_name_gender(file_path):\n","    df = pd.read_csv(file_path, header=None, encoding='utf-8')\n","    name_gender_count_dict = dict(zip(df.iloc[:, 0].str.capitalize().str.strip(), zip(df.iloc[:, 1], df.iloc[:, 2])))\n","    return name_gender_count_dict\n","\n","# Process CSV files and update the unique names and genders dictionaries\n","unique_names = set()  # Initialize set to store unique names\n","name_gender_dict = {}  # Initialize dictionary to store names and genders\n","\n","for file_path in file_paths_csv:\n","    name_gender_count_dict = read_csv_name_gender(file_path)\n","    for name, (gender, count) in name_gender_count_dict.items():\n","        if name in name_gender_dict:\n","            # If the name already exists in the dictionary, update the gender if the count is higher\n","            if count > name_gender_dict[name][1]:\n","                name_gender_dict[name] = (gender, count)\n","        else:\n","            # If the name is not in the dictionary, add it\n","            name_gender_dict[name] = (gender, count)\n","        # Update unique_names\n","        unique_names.add(name)"]},{"cell_type":"markdown","source":["Atribute gender to Character Names with database"],"metadata":{"id":"QqcgK_7ysXQU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvx4IrpJrqLD","executionInfo":{"status":"aborted","timestamp":1722686056675,"user_tz":-60,"elapsed":8724,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["names_with_gender = []\n","names_without_gender = []\n","\n","# Iterate through each sequence in matching_sequences\n","for sequence in matching_sequences:\n","    # Split the sequence into words\n","    words = sequence.split()\n","\n","    # Extract the first word\n","    first_word = words[0]\n","\n","    # Check if the first word is in the name_gender_dict\n","    if first_word in name_gender_dict:\n","        # Get the gender of the first word\n","        gender, _ = name_gender_dict[first_word]\n","\n","        # Append the entire entry (sequence) with its gender\n","        names_with_gender.append((sequence, gender))\n","    else:\n","        # Append only the first word if its gender is not found\n","        names_without_gender.append(first_word)\n","\n","# Print the lists to verify\n","print(\"Names with gender:\", names_with_gender)\n","print(\"Names without gender:\", names_without_gender)"]},{"cell_type":"markdown","metadata":{"id":"5DrEAH0Yugfr"},"source":["For the remaining names that were not on the data base we use a system that retrieves tokens that antecede the name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bN4uZCq0sYu1","executionInfo":{"status":"aborted","timestamp":1722686056675,"user_tz":-60,"elapsed":8721,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["\n","# Function to safely evaluate the cell's content and return it as a Python object\n","def safe_eval(cell):\n","    try:\n","        return literal_eval(cell)\n","    except (ValueError, SyntaxError):\n","        # Return None if the cell content is not a valid Python literal\n","        return None\n","\n","# Function to extract all tokens and tags from the cell\n","def extract_info(cell):\n","    tokens_tags = []  # Initialize an empty list to hold (token, tag) tuples\n","    if pd.notna(cell):\n","        cell_list = safe_eval(cell)\n","        if cell_list:\n","            # Process each dictionary in the list\n","            for item in cell_list:\n","                form = item.get('form')\n","                pos = item.get('pos')\n","                tokens_tags.append((form, pos))  # Add the (token, tag) tuple to the list\n","    return tokens_tags\n","\n","# Dictionary to hold the results\n","results = {sequence: [] for sequence in names_without_gender}\n","\n","# Iterate through each row and cell in the DataFrame\n","for index, row in ltags.iterrows():\n","    for cell in row:\n","        tokens_tags = extract_info(cell)  # Extract tokens and their tags\n","        for i, (token, tag) in enumerate(tokens_tags):\n","            if token in names_without_gender:\n","                # Check if the previous two tokens exist and if either of them is 'DA' or 'STT'\n","                if i > 0:\n","                    if tokens_tags[i-1][1] in ['DA', 'STT']:\n","                        results[token].append(tokens_tags[i-1][0])\n","                    if i > 1 and tokens_tags[i-2][1] in ['DA', 'STT']:\n","                        results[token].append(tokens_tags[i-2][0])\n","\n","# Now, print out the results\n","for sequence, prev_tokens in results.items():\n","    prev_tokens_str = '\", \"'.join(str(token) for token in prev_tokens)\n","    print(f'\"{sequence}\": \"{prev_tokens_str}\"')"]},{"cell_type":"markdown","source":["Voting system takes place using the gender of determinants and titles to achieve most likely gender"],"metadata":{"id":"C-FkD0JltdtS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q51OhK7q3GDO","executionInfo":{"status":"aborted","timestamp":1722686056675,"user_tz":-60,"elapsed":8718,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["# Define the gender categories\n","female_tokens = [\"sra.\", \"da.\", \"a\", \"as\", \"senhora\", \"santa\"]\n","male_tokens = [\"sr.\", \"o\", \"os\", \"senhor\", \"santo\"]\n","\n","# Initialize a dictionary to hold the counts\n","gender_counts = {name: {'Female': 0, 'Male': 0, 'Genderless': 0} for name in results.keys()}\n","\n","# Iterate over the results to categorize and count the preceding tokens\n","for name, tokens in results.items():\n","    for token in tokens:\n","        # Normalize token for case-insensitive comparison\n","        token_normalized = token.lower()\n","        if token_normalized in female_tokens:\n","            gender_counts[name]['Female'] += 1\n","        elif token_normalized in male_tokens:\n","            gender_counts[name]['Male'] += 1\n","        else:\n","            gender_counts[name]['Genderless'] += 1\n","\n","# Print the results\n","for name, counts in gender_counts.items():\n","    total_tokens = sum(counts.values())\n","    print(f\"{name}: Female - {counts['Female']}  |  Male - {counts['Male']}  |  Genderless - {counts['Genderless']}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qoTuo9beT6Xa","executionInfo":{"status":"aborted","timestamp":1722686056676,"user_tz":-60,"elapsed":8715,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["final_gender_determination = []\n","\n","for name, counts in gender_counts.items():\n","\n","    if counts['Female'] > counts['Male']:\n","        final_gender = 'F'\n","    elif counts['Male'] > counts['Female']:\n","        final_gender = 'M'\n","    else:\n","        final_gender = '-'\n","    final_gender_determination.append((name, final_gender))\n","\n","for item in final_gender_determination:\n","    print(f\"Name: {item[0]}, Gender: {item[1]}\")"]},{"cell_type":"markdown","metadata":{"id":"AZAkXIH6JM9H"},"source":["We now apply the same process to the sequences that were cleaned but didnt match the name database, we do this so we can use them in future work in case its needed ex. Potential Co-reference work"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsenXFQiJM9K","executionInfo":{"status":"aborted","timestamp":1722686056676,"user_tz":-60,"elapsed":8711,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["\n","# Function to safely evaluate the cell's content and return it as a Python object\n","def safe_eval(cell):\n","    try:\n","        return literal_eval(cell)\n","    except (ValueError, SyntaxError):\n","        # Return None if the cell content is not a valid Python literal\n","        return None\n","\n","# Function to extract all tokens and tags from the cell\n","def extract_info(cell):\n","    tokens_tags = []  # Initialize an empty list to hold (token, tag) tuples\n","    if pd.notna(cell):\n","        cell_list = safe_eval(cell)\n","        if cell_list:\n","            # Process each dictionary in the list\n","            for item in cell_list:\n","                form = item.get('form')\n","                pos = item.get('pos')\n","                tokens_tags.append((form, pos))  # Add the (token, tag) tuple to the list\n","    return tokens_tags\n","\n","# Dictionary to hold the results\n","results_non_match = {sequence: [] for sequence in non_matching_sequences}\n","\n","# Iterate through each row and cell in the DataFrame\n","for index, row in ltags.iterrows():\n","    for cell in row:\n","        tokens_tags = extract_info(cell)  # Extract tokens and their tags\n","        for i, (token, tag) in enumerate(tokens_tags):\n","            if token in non_matching_sequences:\n","                # Check if the previous two tokens exist and if either of them is 'DA' or 'STT'\n","                if i > 0:\n","                    if tokens_tags[i-1][1] in ['DA', 'STT']:\n","                        results_non_match[token].append(tokens_tags[i-1][0])\n","                    if i > 1 and tokens_tags[i-2][1] in ['DA', 'STT']:\n","                        results_non_match[token].append(tokens_tags[i-2][0])\n","\n","# Now, print out the results\n","for sequence, prev_tokens in results_non_match.items():\n","    prev_tokens_str = '\", \"'.join(prev_tokens)\n","    print(f'\"{sequence}\": \"{prev_tokens_str}\"')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_9Zpa4MJM9N","executionInfo":{"status":"aborted","timestamp":1722686056676,"user_tz":-60,"elapsed":8708,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["# Initialize a dictionary to hold the counts\n","non_matching_gender_counts = {name: {'Female': 0, 'Male': 0, 'Genderless': 0} for name in results_non_match.keys()}\n","\n","# Iterate over the results to categorize and count the preceding tokens\n","for name, tokens in results_non_match.items():\n","    for token in tokens:\n","        # Normalize token for case-insensitive comparison\n","        token_normalized = token.lower()\n","        if token_normalized in female_tokens:\n","            non_matching_gender_counts[name]['Female'] += 1\n","        elif token_normalized in male_tokens:\n","            non_matching_gender_counts[name]['Male'] += 1\n","        else:\n","            non_matching_gender_counts[name]['Genderless'] += 1\n","\n","# Print the results\n","for name, counts in non_matching_gender_counts.items():\n","    total_tokens = sum(counts.values())\n","    print(f\"{name}: Female - {counts['Female']}  |  Male - {counts['Male']}  |  Genderless - {counts['Genderless']}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YL9kXTKhVSFF","executionInfo":{"status":"aborted","timestamp":1722686056677,"user_tz":-60,"elapsed":8706,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["final_gender_determination2 = []\n","\n","for name, counts in non_matching_gender_counts.items():\n","\n","    if counts['Female'] > counts['Male']:\n","        final_gender = 'F'\n","    elif counts['Male'] > counts['Female']:\n","        final_gender = 'M'\n","    else:\n","        final_gender = '-'\n","    final_gender_determination2.append((name, final_gender))\n","\n","for item in final_gender_determination2:\n","    print(f\"Name: {item[0]}, Gender: {item[1]}\")"]},{"cell_type":"markdown","source":["We create a gender dictionary that can be called in to use when needed that covers Definitive names and possible names"],"metadata":{"id":"nBZvJ2Zkuuot"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8viUyTkHFrBf","executionInfo":{"status":"aborted","timestamp":1722686056677,"user_tz":-60,"elapsed":8703,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["# Create an empty dictionary to hold the combined gender information\n","gender_dictionary = {}\n","\n","# Update gender_dictionary with final_gender_determination2\n","gender_dictionary.update(final_gender_determination2)\n","\n","# Update gender_dictionary with final_gender_determination\n","gender_dictionary.update(final_gender_determination)\n","\n","# Update gender_dictionary with names_with_gender\n","gender_dictionary.update(names_with_gender)\n","\n","# Print the combined gender dictionary\n","print(gender_dictionary)"]},{"cell_type":"markdown","metadata":{"id":"Uu2Ywjl9BrsV"},"source":["# Co-Reference Resolution"]},{"cell_type":"markdown","source":["Pre-processing names for LX-tagger and Spacy Compatibility"],"metadata":{"id":"kELzRtY3vMce"}},{"cell_type":"code","source":["#Normalize names for Spacy\n","corrected_names = []\n","for name in matching_sequences:\n","    corrected_name = name.replace(\"de_ a\", \"da\").replace(\"de_ os\", \"dos\").replace(\"de _ a\", \"da\").replace(\"de _ os\", \"dos\").replace(\"de _ o\", \"do\").replace(\"de_ o\", \"do\").replace(\"de _ as\", \"das\").replace(\"de_ as\", \"das\")\n","    corrected_names.append(corrected_name)\n","\n","matching_sequences = corrected_names"],"metadata":{"id":"wO5MpOXgFvPI","executionInfo":{"status":"aborted","timestamp":1722686056677,"user_tz":-60,"elapsed":8702,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6yyLtXGiOyIY"},"source":["### Heuristics"]},{"cell_type":"markdown","source":["See documents for more detailed information about this step"],"metadata":{"id":"WM7dL_zj0kel"}},{"cell_type":"markdown","source":["Starting with multiple word tokens, we start with the entries with the bigger number of tokens and create a group with them, we iterate through every entry with multiple tokens in descending order, if there is a 100% match with tokens from a previous entry, we join them togheter, if no match is found a new group is created, this way we create groups with the possible variations of portuguese full names"],"metadata":{"id":"1ea6jVAc0nSs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ls6EoCY58PHt","executionInfo":{"status":"aborted","timestamp":1722686056677,"user_tz":-60,"elapsed":8698,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["\n","# Parse names\n","parsed_names = [HumanName(name) for name in matching_sequences]\n","\n","# Sort names by the number of tokens in descending order\n","sorted_names = sorted(parsed_names, key=lambda x: len(x.full_name.split()), reverse=True)\n","\n","# Group names based on token presence, excluding single-word tokens\n","name_groups = []\n","for name in sorted_names:\n","    # Skip names with only one token\n","    if len(name.full_name.split()) <= 1:\n","        continue\n","\n","    found = False\n","    for group in name_groups:\n","        if all(token in group['names'][0].full_name.split() for token in name.full_name.split()):\n","            group['names'].append(name)\n","            group['counter'] += 1\n","            found = True\n","            break\n","    if not found:\n","        name_groups.append({'names': [name], 'counter': 1})\n","\n","# Determine group names and find the longest name for each group\n","for group in name_groups:\n","    token_frequency = Counter()\n","    for name in group['names']:\n","        tokens = name.full_name.split()\n","        token_frequency.update(tokens)\n","\n","    # Iterate over most common tokens and select the first one in unique_names\n","    common_token = None\n","    for token, count in token_frequency.most_common():\n","        if token in unique_names:\n","            common_token = token\n","            break\n","\n","    # If no common token is found in unique_names, default to the first name's first token or \"Unnamed Group\"\n","    common_token = common_token if common_token else group['names'][0].full_name.split()[0] if group['names'] else \"Unnamed Group\"\n","\n","    # Find the longest name starting with the common token\n","    longest_name = ''\n","    for name in group['names']:\n","        if name.full_name.startswith(common_token) and len(name.full_name) > len(longest_name):\n","            longest_name = name.full_name\n","\n","    # Extract the last token of the longest name\n","    last_token = longest_name.split()[-1] if longest_name else ''\n","\n","    # Form the new group name by combining the common token and the last token, avoiding repetition\n","    group['common_name'] = f\"{common_token} {last_token}\" if common_token and last_token and common_token != last_token else common_token\n","\n","final_groups = name_groups\n","\n","# Print each group with the new naming convention\n","for idx, group in enumerate(name_groups):\n","    group_number = idx + 1\n","    common_name = group.get('common_name', 'Unnamed Group')\n","    group_counter = group['counter']\n","    group_names = [name.full_name for name in group['names']]\n","    print(f\"Group {group_number}: {common_name} (Count: {group_counter}): {group_names}\")\n"]},{"cell_type":"markdown","metadata":{"id":"NmtAVcs7Pks7"},"source":["We then count the number of appearences of each group in order to determine relevance in the book"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMf6TG7SJvZp","executionInfo":{"status":"aborted","timestamp":1722686056678,"user_tz":-60,"elapsed":8696,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["\n","# Modified function to handle spaCy Span objects\n","def find_group_name_in_sentence(sentence, group_list, exclude_word=''):\n","    found_group_names = set()\n","    found_name_details = []  # To store found names and their lengths\n","\n","    # Convert the Span to string and exclude the specific word for matching\n","    modified_sentence_text = ' '.join(word.text for word in sentence if word.text != exclude_word)\n","\n","    # Flatten and sort the names from all groups by length (descending)\n","    all_names = [(group['common_name'], name) for group in group_list for name in group['names']]\n","    sorted_all_names = sorted(all_names, key=lambda x: len(x[1].full_name), reverse=True)\n","\n","    for group_name, name_obj in sorted_all_names:\n","        full_name = name_obj.full_name\n","        # Compile a regex pattern to search for the name as a whole word\n","        pattern = re.compile(r'\\b' + re.escape(full_name) + r'\\b', re.IGNORECASE)\n","\n","        # Check the modified sentence text without the excluded word\n","        if pattern.search(modified_sentence_text):\n","            # Check if this name is part of a longer name already found\n","            if not any(full_name in found for found, _ in found_name_details):\n","                found_group_names.add(group_name)\n","                found_name_details.append((full_name, len(full_name)))\n","\n","    return found_group_names\n","\n","group_name_counts = Counter()\n","\n","# Process each sentence in the document\n","for sentence in book.sents:\n","    matched_groups = find_group_name_in_sentence(sentence, final_groups)  # Adjusted to expect a single return value\n","    group_name_counts.update(matched_groups)\n","\n","sorted_counts = sorted(group_name_counts.items(), key=lambda x: x[1], reverse=True)\n","\n","# Print the sorted items\n","for group_name, count in sorted_counts:\n","    print(f\"Group Name: '{group_name}' - Count: {count}\")"]},{"cell_type":"markdown","metadata":{"id":"75sT3LhaPs8h"},"source":["Now we treat single word tokens, we prioritize matching a single entry if it matches the first name of a character, when multiple options are preseanted we atribute the token to the most relevant group since its ost likely to be a reference to it, the same relavance as a tiebreaker is applied for tokens that correspond to other parts of the name if no possible match is found we consider it a new character"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qZnis-eIfKiT","executionInfo":{"status":"aborted","timestamp":1722686056678,"user_tz":-60,"elapsed":8693,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["\n","single_token_names = [name for name in matching_sequences if len(name.split()) == 1]\n","unmatched_names = []\n","\n","for single_name in single_token_names:\n","    potential_groups = []\n","\n","    # Try matching the first token of the name first\n","    first_token = single_name.split()[0]\n","    potential_groups.extend([group for group in final_groups if any(first_token in name.full_name.split()[0] for name in group['names'])])\n","\n","    # If no potential groups are found, try matching every token\n","    if not potential_groups:\n","        potential_groups.extend([group for group in final_groups if any(single_name in name.full_name for name in group['names'])])\n","\n","    if potential_groups:\n","        # Allocate to the best matching group based on previous logic\n","        sorted_potential_groups = sorted(potential_groups, key=lambda x: group_name_counts[x['common_name']], reverse=True)\n","        best_group = sorted_potential_groups[0]\n","    else:\n","        # Create a new group if no matching group is found\n","        best_group = {'common_name': single_name, 'counter': 0, 'names': []}\n","        final_groups.append(best_group)\n","        unmatched_names.append(single_name)\n","\n","    # Add the single-token name to the best group\n","    best_group['names'].append(HumanName(single_name))\n","    best_group['counter'] += 1\n","    group_name_counts[best_group['common_name']] = best_group['counter']\n","\n","### Step 2: Print the Final Groups with All Names\n","\n","for idx, group in enumerate(final_groups):\n","    group_number = idx + 1\n","    common_name = group['common_name']\n","    group_counter = group['counter']\n","    group_names = [name.full_name for name in group['names']]  # Extract the full name correctly\n","    print(f\"Group {group_number}: {common_name} (Count: {group_counter}): {group_names}\")\n","\n","print(\"Unmatched Names:\", unmatched_names)"]},{"cell_type":"markdown","source":["We again count appearences in the book for relevance purposes"],"metadata":{"id":"o6MQnbsB5jTm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MePItuMk7zFZ","executionInfo":{"status":"aborted","timestamp":1722686057231,"user_tz":-60,"elapsed":4,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["group_name_counts = Counter()\n","\n","# Process each sentence in the document\n","for sentence in book.sents:\n","    matched_groups = find_group_name_in_sentence(sentence, final_groups)  # Adjusted to expect a single return value\n","    group_name_counts.update(matched_groups)\n","\n","sorted_counts = sorted(group_name_counts.items(), key=lambda x: x[1], reverse=True)\n","\n","# Print the sorted items\n","for group_name, count in sorted_counts:\n","    print(f\"Group Name: '{group_name}' - Count: {count}\")"]},{"cell_type":"markdown","source":["### Nicknames"],"metadata":{"id":"68NsZ0-i554A"}},{"cell_type":"markdown","source":["Portuguese nicknames list"],"metadata":{"id":"dlP4VVLR6Ju1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nByHDK8s3MMI","executionInfo":{"status":"aborted","timestamp":1722686057231,"user_tz":-60,"elapsed":4,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["\n","dados = {\n","      \"Nomes\": [\n","        \"Afonso\", \"Alexandra\", \"Alexandre\", \"Alfredo\", \"Alice\",\n","        \"Amélia\", \"Ana\", \"Antônio/António\", \"Bárbara\", \"Bianca\",\"Bento\",\n","        \"Camila\", \"Carlos\", \"Carlota\", \"Carolina\", \"Cecília\", \"Clara\",\n","        \"Cláudia\", \"Cristina\", \"Daniel\", \"Daniela\", \"Diana\",\n","        \"Diogo\", \"Eduardo/Eduarda\", \"Elisabete\", \"Elvira\", \"Emília\",\n","        \"Fábio/Fabiano\", \"Fernando\", \"Fernanda\", \"Filipa\",\n","        \"Filipe\", \"Filomena\", \"Francisca\", \"Francisco\",\n","        \"Frederico\", \"Gabriel\", \"Gabriela\", \"Gonçalo\",\n","        \"Guilherme\", \"Gustavo\", \"Helena\", \"Henrique\",\n","        \"Horácio\", \"Inês\", \"Isabel\", \"Jaime\",\n","        \"Jéssica\", \"Joana\", \"João\", \"Joaquim\", \"Jorge\", \"José\",\n","        \"Júlia\", \"Juliana\", \"Laura\", \"Leonardo\", \"Leonor\",\n","        \"Lídia\", \"Lígia\", \"Liliana\", \"Lorena*\", \"Luís\",\n","        \"Luísa\", \"Lúcia\", \"Madalena\", \"Magda\", \"Manuel\",\n","        \"Manuela\", \"Marcos\", \"Margarida\", \"Maria\", \"Mariana\",\n","        \"Mário\", \"Marta\", \"Micael/Micaela\", \"Miguel\", \"Nélson\",\n","        \"Nicolau\", \"Nicole\", \"Otávio\", \"Osvaldo\", \"Patrícia\",\n","        \"Paula\", \"Paulo\", \"Pedro\", \"Rafael\", \"Renata\", \"Ricardo\",\n","        \"Rita\", \"Roberto\", \"Rodrigo\", \"Rodolfo\", \"Rosa\", \"Rui\",\n","        \"Sebastião\", \"Sérgio\", \"Sofia\", \"Susana\", \"Teresa\", \"Tiago\",\n","        \"Tomé\", \"Vanessa\", \"Vera\", \"Vítor\", \"Vitória\"\n","    ],\n","\n","    \"Diminutivo/Alcunha\": [\n","                \"Afonsinho, Fonsinho, Fofó\",\n","        \"Xana, Alex, Xanda, Lexi, Alexia\",\n","        \"Alex, Xande, Xandinho\",\n","        \"Fred, Alfrinho\",\n","        \"Alicinha, Licinha, Cinha, Lice, Lili\",\n","        \"Amelinha, Melita, Mel, Lia, Mili, Amy\",\n","        \"Aninha, Anita, Anoca(s), Nita, Ninha, Nana, Aninhas\",\n","        \"Tó, Tony, Toninho\",\n","        \"Bá, Babi, Barbi, Babinha\",\n","        \"Bia, Bibi, Bi, Bibis, Bianquinha\",\n","        \"Bentinho\",\n","        \"Camilinha, Mila, Mia, Cammy, Cam, Mi, Milinha\",\n","        \"Carlinhos, Carlitos, Cacá, Carlão\",    \"Lota\",\n","        \"Carolininha, Carol, Carola, Lina\",\n","        \"Cilinha, Cila, Cissa, Ceci, Cissy\",\n","        \"Clarinha\",\n","        \"Claudinha, Clau\",\n","        \"Cris, Cristininha, Tina, Cristy\",\n","        \"Dani, Dan, Danny\",\n","        \"Dani, Danizinha, Danni\",\n","        \"Di, Didi, Dianinha\",    \"Dioguinho, Dioguito, Di, Didi\",\n","        \"Edu, Dudu, Duda, Du\",  \"Bete, Elisa, Bé, Betinha, Lisa, Beta, Eli\",\n","        \"Elvirinha, Vira\",\n","        \"Emilinha, Mila, Milinha, Mili, Lia, Emi\",   \"Fafá, Biano, Fabí, Fá\",\n","        \"Fernandinho, Nando, Fanã\",\n","        \"Nanda, Nandinha, Fê\",\n","        \"Filipinha, Lipa, Pipa, Fifi\",\n","        \"Filipinho, Lipe, Pipo\",\n","        \"Mena, Filó\",\n","        \"Francisquinha, Chica, Chiquinha, Kika, Kikinha\",\n","        \"Francisquinho, Chico, Chiquinho, Chiquito, Kiko, Kikinho, Cisco\",\n","        \"Fred\",         \"Gabi, Biel\", \"Gabi\",\n","        \"Gonçalinho, Gonça, Gongas\",      \"Gui\",\n","        \"Guto, Guga\",\n","        \"Lena, Leninha, Leni\",    \"Riquinho, Harry, Henri\",    \"Horacinho\",\n","        \"Inesinha, Nê, Nenê, Nês, Nenuca\",\n","        \"Isabelinha, Isabelita, Belinha, Isa, Bel, Bela, Bê\",\n","        \"Jaiminho, Jaimão\",\n","        \"Jecas, Jessie, Jê, Jess\",\n","        \"Joaninha, Ju, Juju, Jana, Janocas, Joanocas, Jô\",\n","        \"Joãozinho, Janjão, Jão, Juca, Joca, Janocas, JJ\",\n","        \"Quim, Joca, Jaquim, Quinzinho\",\n","        \"Jorginho, Jó\",\n","        \"Zé, Zezé, Zeca, Zezinho, Zequinha\",\"Ju, Julinha, Juju, Lia\",\n","        \"Ju, Juju, Liana, Li\",\n","        \"Laurinha, Lara\",\n","        \"Leo, Leozinho\",\n","        \"Nonô, Leo, Nonocas\",\n","        \"Lídi, Li, Dida, Lili\",\n","        \"Gia, Gigi\",\n","        \"Lili, Lilas\",\n","        \"Lora, Ló\",\n","        \"Lu, Luisinho, Lula*, Lulu*\",\n","        \"Luisinha, Lu, Isa\",\n","        \"Lucinha, Lucy, Lu\",\n","        \"Lena, Maddie\",\n","        \"Magdinha\",\n","        \"Manelinho, Manelocas, Manel, Mané, Maneco, Nelo, Nelito, Nelinho\",\n","        \"Manela, Manu, Nelinha, Manocas, Nela\",\n","        \"Marquinhos, Macos\",\n","        \"Guidinha, Guida, Guidinha, Maggie, Gui, Margô\",\n","        \"Mariazinha, Mia, Mimi, Mary\",\n","        \"Marianinha, Marianita, Nita, Mari, Micas, Mary, Mimi\",\n","        \"Marinho, Mári\",\n","        \"Martinha\",\n","        \"Mica, Micas\",\n","        \"Miguelinho, Miguelito\",\n","        \"Nelo, Nelinho, Nelito\",\n","        \"Nico\",\n","        \"Nick, Ni\",\n","        \"Távio\",\n","        \"Valdinho, Vavá, Valdo\",\n","        \"Patty, Pat, Trícia/Trixie\",\n","        \"Paulinha\",\n","        \"Paulinho, Paulão\",\n","        \"Pedrinho, Pedrito, Pedrocas, Pepê\",\n","        \"Rafa, Rafinha\",\n","        \"Rê, Renatinha\",\n","        \"Ricardinho, Rico, Rick, Ricky\",\n","        \"Ritinha, Riri\",\n","        \"Betinho, Berto, Beto, Robbie, Bob\",\n","        \"Rudy, Rod, Rui, Ruy, Rodriguinho\",\n","        \"Rena\",\n","        \"Rosinha\",\n","        \"Ruizinho, Ruizão\",\n","        \"Sebastiãozinho, Tião, Sebas\",\n","        \"Serginho\",\n","        \"Sofi, Sofs\",\n","        \"Susaninha, Su, Suse, Susie\",\n","        \"Teresinha\",\n","        \"Tiaguinho, Ti, Tigas\",\n","        \"Tomezinho\",\n","        \"Vanessinha, Nessa, Nessie, Nê\",\n","        \"Verinha, Veroca, Verushka\",\n","        \"Vitinho, Vic, Vitão\",\n","        \"Vivi, Vicky, Vick\"\n","    ]\n","\n","}\n","\n","# Create DataFrame\n","df_alcunhas= pd.DataFrame(dados)\n","\n","df_alcunhas\n"]},{"cell_type":"markdown","source":["We first iterate trough the groups and find potential nicknames"],"metadata":{"id":"kxfsg1Sc7ZUw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"G7NvnymE4Ojs","executionInfo":{"status":"aborted","timestamp":1722686057232,"user_tz":-60,"elapsed":5,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["def check_group_common_name_token_in_diminutivo(final_groups, df_alcunhas):\n","    for idx, group in enumerate(final_groups):\n","        group_number = idx + 1\n","        common_name = group['common_name']\n","        first_token = common_name.split()[0]  # Selecting the first token of the common name\n","        group_counter = group['counter']\n","\n","        # Checking if the first token is present in the 'Diminutivo/Alcunha' column\n","        matched_row = df_alcunhas[df_alcunhas['Diminutivo/Alcunha'].apply(lambda x: first_token in x.split(', '))]\n","\n","        if not matched_row.empty:\n","            print(f\"Group {group_number}: {common_name} (Count: {group_counter}) first token '{first_token}' is present in 'Diminutivo/Alcunha'.\")\n","\n","# Call the function with your final_groups and DataFrame\n","check_group_common_name_token_in_diminutivo(final_groups, df_alcunhas)\n"]},{"cell_type":"markdown","source":["We now change the first name in Groups if its a nickname and if it matches the name of another group we merge them"],"metadata":{"id":"m8_k7Xzg7z21"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1y__xnbzLnfu","executionInfo":{"status":"aborted","timestamp":1722686057232,"user_tz":-60,"elapsed":5,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["# Iterate through all groups\n","for group in final_groups:\n","    common_name = group['common_name']\n","    first_token = common_name.split()[0]  # Extract the first token\n","    # Check if there's a second token\n","    if len(common_name.split()) > 1:\n","        second_token = common_name.split()[1]\n","    else:\n","        second_token = None\n","    # Find a row in df_alcunhas where the first token matches any name in 'Diminutivo/Alcunha'\n","    matched_rows = df_alcunhas[df_alcunhas['Diminutivo/Alcunha'].str.contains(first_token, regex=False)]\n","    if not matched_rows.empty:\n","        # If a match is found, update the common name to the corresponding 'Nomes' entry\n","        new_common_name = matched_rows.iloc[0]['Nomes']\n","        # If there's a second token, append it to the new common name\n","        if second_token:\n","            new_common_name += ' ' + second_token\n","        group['common_name'] = new_common_name\n","\n","# Create a defaultdict to store the merged groups\n","merged_groups = defaultdict(lambda: {'counter': 0, 'names': []})\n","\n","# Iterate through all groups (including those not updated)\n","for group in final_groups:\n","    # Update the merged_groups dictionary with the current group\n","    merged_groups[group['common_name']]['counter'] += group['counter']\n","    merged_groups[group['common_name']]['names'].extend(group['names'])\n","\n","# Filter out empty groups and convert the dictionary back into a list of dictionaries\n","merged_final_groups = [{'common_name': key, 'counter': value['counter'], 'names': value['names']} for key, value in merged_groups.items() if value['counter'] > 0]  #Print the merged final groups"]},{"cell_type":"markdown","source":["Re-print groups to look at results"],"metadata":{"id":"QTBelhD38RYK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ddX-hY9rMOS0","executionInfo":{"status":"aborted","timestamp":1722686057232,"user_tz":-60,"elapsed":5,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["for idx, group in enumerate(merged_final_groups):\n","    group_number = idx + 1\n","    common_name = group['common_name']\n","    group_counter = group['counter']\n","    group_names = [name['full_name'] for name in group['names']]  # Extract the full name correctly\n","    print(f\"Group {group_number}: {common_name} (Count: {group_counter}): {group_names}\")"]},{"cell_type":"markdown","source":["##Is the book in 1ºst person?"],"metadata":{"id":"VDLcB2zgVq7i"}},{"cell_type":"markdown","source":["If the book i not in first person ignore"],"metadata":{"id":"JFxtbbDaVyJ-"}},{"cell_type":"code","source":["# Prompt the user to input the group number to add \"eu\"\n","group_number_input = input(\"Enter the group number to add 'eu' (or type 'skip' to continue without adding): \")\n","\n","# Check if the user wants to add \"eu\" to a group\n","if group_number_input.lower() != 'skip':\n","    try:\n","        group_number = int(group_number_input)\n","        if 1 <= group_number <= len(merged_final_groups):\n","            # Add \"eu\" to the selected group\n","            selected_group = merged_final_groups[group_number - 1]\n","            selected_group['names'].append({'full_name': 'eu'})  # Add \"eu\" to the names list\n","            print(f\"'eu' added to Group {group_number}: {selected_group['common_name']}\")\n","        else:\n","            print(\"Invalid group number.\")\n","    except ValueError:\n","        print(\"Invalid input. Please enter a valid group number.\")"],"metadata":{"id":"jsOIDe0TVwCf","executionInfo":{"status":"aborted","timestamp":1722686057232,"user_tz":-60,"elapsed":4,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Detect Interactions"],"metadata":{"id":"mVTvvNIXJ9Kg"}},{"cell_type":"markdown","source":["###Pre-processing"],"metadata":{"id":"yDRi8GVfKCt1"}},{"cell_type":"markdown","source":["Exclude Characters that appear less then 3 times for irelevance or mistakes in the text"],"metadata":{"id":"63D0GGp4_AVS"}},{"cell_type":"code","source":["def find_group_name_in_sentence(sentence, group_list, exclude_word=''):\n","    found_group_names = set()\n","    found_name_details = []  # To store found names and their lengths\n","\n","    # Convert the sentence Span to string and exclude the specific word for matching\n","    modified_sentence_text = ' '.join(word.text for word in sentence if word.text != exclude_word)\n","\n","    # Flatten and sort the names from all groups by length (descending)\n","    all_names = [(group['common_name'], name['full_name']) for group in group_list for name in group['names']]\n","    sorted_all_names = sorted(all_names, key=lambda x: len(x[1]), reverse=True)\n","\n","    for group_name, full_name in sorted_all_names:\n","        # Compile a regex pattern to search for the name as a whole word\n","        pattern = re.compile(r'\\b' + re.escape(full_name) + r'\\b', re.IGNORECASE)\n","\n","        # Check the modified sentence text without the excluded word\n","        if pattern.search(modified_sentence_text):\n","            # Check if this name is part of a longer name already found\n","            if not any(full_name in found for found, _ in found_name_details):\n","                found_group_names.add(group_name)\n","                found_name_details.append((full_name, len(full_name)))\n","\n","    return found_group_names\n","\n","def preprocess_names(merged_final_groups):\n","    renamed_groups = {}  # Dictionary to store renamed groups\n","    single_token_lowercase_groups = set()  # Set to store single-token, lowercase group names\n","\n","    for group in merged_final_groups:\n","        common_name = group['common_name']\n","        if len(common_name.split()) == 1:  # Single token group\n","            if common_name.islower():  # Check if the single token is lowercase\n","                single_token_lowercase_groups.add(common_name)\n","            renamed_groups[common_name] = common_name\n","        else:\n","            common_name_tokens = common_name.split()\n","            # Select the two most common tokens used to designate the group\n","            top_tokens = ' '.join(sorted(common_name_tokens, key=lambda x: -common_name_tokens.count(x))[:2])\n","            renamed_groups[common_name] = top_tokens\n","\n","    # Update names in merged_final_groups with renamed versions\n","    for group in merged_final_groups:\n","        group['common_name'] = renamed_groups.get(group['common_name'], group['common_name'])\n","\n","    return renamed_groups, single_token_lowercase_groups\n","\n","renamed_groups, single_token_lowercase_groups = preprocess_names(merged_final_groups)  # Dictionary with renamed groups\n","\n","# Initialize counters\n","group_counts = Counter()\n","excluded_names = []\n","\n","# Iterate over each sentence in the document\n","for sentence in book.sents:\n","    # Find group names in the sentence and update the counter\n","    found_groups = find_group_name_in_sentence(sentence, merged_final_groups)\n","    group_counts.update(found_groups)\n","\n","# Iterate over the counts to filter out names occurring less than 3 times\n","for group_name, count in group_counts.items():\n","    if count < 3:\n","        # Move the group name to the excluded names list\n","        excluded_names.append(group_name)\n","\n","# Remove the group names that are either excluded, not found at all, or are single-token lowercase words\n","merged_final_groups = [\n","    group for group in merged_final_groups\n","    if group['common_name'] not in excluded_names\n","    and group['common_name'] in group_counts\n","    and group['common_name'] not in single_token_lowercase_groups\n","]\n","\n","# Print the counts for each group in alphabetical order\n","for group_name, count in sorted(group_counts.items()):\n","    if count >= 3 and group_name not in single_token_lowercase_groups:  # Only print groups that are not excluded\n","        renamed_group_name = renamed_groups.get(group_name, group_name)\n","        print(f\"{renamed_group_name}: {count} occurrences\")\n","\n","# Print the excluded names\n","print(\"Excluded Names:\")\n","for name in sorted(excluded_names):\n","    print(name)"],"metadata":{"id":"dZUAPYP59V-K","executionInfo":{"status":"aborted","timestamp":1722686057233,"user_tz":-60,"elapsed":5,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Print Final Character names"],"metadata":{"id":"kMt-JKmD_sCw"}},{"cell_type":"code","source":["for idx, group in enumerate(merged_final_groups):\n","    group_number = idx + 1\n","    common_name = group['common_name']\n","    group_counter = group['counter']\n","    group_names = [name['full_name'] for name in group['names']]  # Extract the full name correctly\n","    print(f\"Group {group_number}: {common_name} (Count: {group_counter}): {group_names}\")"],"metadata":{"id":"7FZv0ircJeIv","executionInfo":{"status":"aborted","timestamp":1722686057233,"user_tz":-60,"elapsed":5,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1_urYepAEOYC"},"source":["###Co-Ocurrence"]},{"cell_type":"markdown","source":["Perform Co-ocurrence extraction"],"metadata":{"id":"Ek7nq_NAAEtP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IR_aQbywpU3U","executionInfo":{"status":"aborted","timestamp":1722686057233,"user_tz":-60,"elapsed":5,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["from itertools import islice\n","\n","def find_group_name_in_combined_sentences(sentences, group_list, exclude_word=''):\n","    found_group_names = set()\n","    found_name_details = []  # To store found names and their lengths\n","\n","    # Convert the list of sentences to a single string and exclude the specific word for matching\n","    combined_text = ' '.join(word.text for sentence in sentences for word in sentence if word.text != exclude_word)\n","\n","    # Flatten and sort the names from all groups by length (descending)\n","    all_names = [(group['common_name'], name['full_name']) for group in group_list for name in group['names']]\n","    sorted_all_names = sorted(all_names, key=lambda x: len(x[1]), reverse=True)\n","\n","    for group_name, full_name in sorted_all_names:\n","        # Compile a regex pattern to search for the name as a whole word\n","        pattern = re.compile(r'\\b' + re.escape(full_name) + r'\\b', re.IGNORECASE)\n","\n","        # Check the combined text without the excluded word\n","        if pattern.search(combined_text):\n","            # Check if this name is part of a longer name already found\n","            if not any(full_name in found for found, _ in found_name_details):\n","                found_group_names.add(group_name)\n","                found_name_details.append((full_name, len(full_name)))\n","\n","    return found_group_names\n","\n","def combine_sentences(chap, num_sentences=1):\n","    # Create an iterator over the sentences\n","    sentence_iter = iter(chap.sents)\n","\n","    while True:\n","        # Take the next `num_sentences` sentences from the iterator\n","        next_sentences = list(islice(sentence_iter, num_sentences))\n","\n","        # If no more sentences are left, break the loop\n","        if not next_sentences:\n","            break\n","\n","        yield next_sentences\n","\n","# Example usage:\n","num_sentences_to_combine = 3\n","\n","for sentence_group in combine_sentences(book, num_sentences=num_sentences_to_combine):\n","    matched_groups = find_group_name_in_combined_sentences(sentence_group, merged_final_groups)\n","    if matched_groups:\n","        combined_text = ' '.join(sentence.text for sentence in sentence_group)\n","        print(f\"Combined Sentences: '{combined_text}' - Contains Group Names: {', '.join(matched_groups)}\")"]},{"cell_type":"markdown","source":["Build Dataframe of interactions, this can be used for other purposes besides Network construction and allows for further analysis in said networks"],"metadata":{"id":"bQKcrE00AgrV"}},{"cell_type":"code","source":["def find_group_name_in_combined_sentences(sentences, group_list, exclude_word=''):\n","    found_group_names = set()\n","    found_name_details = []  # To store found names and their lengths\n","\n","    # Convert the list of sentences to a single string and exclude the specific word for matching\n","    combined_text = ' '.join(word.text for sentence in sentences for word in sentence if word.text != exclude_word)\n","\n","    # Flatten and sort the names from all groups by length (descending)\n","    all_names = [(group['common_name'], name['full_name']) for group in group_list for name in group['names']]\n","    sorted_all_names = sorted(all_names, key=lambda x: len(x[1]), reverse=True)\n","\n","    for group_name, full_name in sorted_all_names:\n","        # Compile a regex pattern to search for the name as a whole word\n","        pattern = re.compile(r'\\b' + re.escape(full_name) + r'\\b', re.IGNORECASE)\n","\n","        # Check the combined text without the excluded word\n","        if pattern.search(combined_text):\n","            # Check if this name is part of a longer name already found\n","            if not any(full_name in found for found, _ in found_name_details):\n","                found_group_names.add(group_name)\n","                found_name_details.append((full_name, len(full_name)))\n","\n","    return found_group_names\n","\n","def combine_sentences(chap, num_sentences=1):\n","    # Create an iterator over the sentences\n","    sentence_iter = iter(chap.sents)\n","\n","    while True:\n","        # Take the next `num_sentences` sentences from the iterator\n","        next_sentences = list(islice(sentence_iter, num_sentences))\n","\n","        # If no more sentences are left, break the loop\n","        if not next_sentences:\n","            break\n","\n","        yield next_sentences\n","\n","# Initialize an empty counter for interactions\n","interaction_counter = Counter()\n","\n","# Initialize an empty list to store interactions\n","interactions = []\n","\n","# Example usage:\n","num_sentences_to_combine = 3\n","\n","for sentence_group in combine_sentences(book, num_sentences=num_sentences_to_combine):\n","    matched_groups = find_group_name_in_combined_sentences(sentence_group, merged_final_groups)\n","    if len(matched_groups) > 1:\n","        interaction_counter.update(combinations(matched_groups, 2))  # Update the interaction counter\n","        interactions.extend(combinations(matched_groups, 2))  # Store the interaction pairs\n","\n","# Create a DataFrame for interactions\n","interactions_df = pd.DataFrame(interactions, columns=['Character 1', 'Character 2'])\n","\n","# Count the occurrences of each interaction and add it to the DataFrame\n","interactions_df['Count'] = interactions_df.groupby(['Character 1', 'Character 2']).transform('size')\n","\n","# Drop duplicate rows\n","interactions_df.drop_duplicates(inplace=True)\n","\n","# Reset the index\n","interactions_df.reset_index(drop=True, inplace=True)\n","\n","# Display the interactions DataFrame\n","print(\"Interactions DataFrame:\")\n","interactions_df"],"metadata":{"id":"UkhziN3gATeg","executionInfo":{"status":"aborted","timestamp":1722686057233,"user_tz":-60,"elapsed":5,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Extract Graph"],"metadata":{"id":"MzW7jt5yJK25"}},{"cell_type":"markdown","source":["Input Characters as Nodes and interactions as edges, we exclude interactions that occur less than 3 times because of noise but this can be changed according to needs on the analysis, we test here diferent vizualitazions of networks but depending on the size and number of characters in a book gephi might be more apropriate for vizualization"],"metadata":{"id":"pMd4KRcyEvn8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JJ5zxzKVfpAR","executionInfo":{"status":"aborted","timestamp":1722686057234,"user_tz":-60,"elapsed":6,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["df = interactions_df\n","# Create an empty graph\n","G = nx.Graph()\n","\n","# Add nodes and weighted edges to the graph\n","for index, row in df.iterrows():\n","    group1 = row[\"Character 1\"]\n","    group2 = row[\"Character 2\"]\n","    count = row[\"Count\"]\n","    G.add_edge(group1, group2, weight=count)\n","\n","# Remove nodes with degree less than 3\n","nodes_to_remove = [node for node, degree in dict(G.degree()).items() if degree < 3]\n","G.remove_nodes_from(nodes_to_remove)\n","\n","# Calculate centrality measures for nodes\n","centrality = nx.degree_centrality(G)\n","\n","# Draw the network graph\n","pos = nx.spring_layout(G)  # Positions of nodes\n","\n","# Draw nodes with size proportional to centrality\n","node_size = [centrality[node] * 2000 for node in G.nodes()]\n","\n","# Draw edges with thickness proportional to weight\n","edge_width = [d[\"weight\"] / 5 for u, v, d in G.edges(data=True)]\n","\n","nx.draw(G, pos, with_labels=True, node_size=node_size, node_color=\"skyblue\", font_size=10, width=edge_width)\n","plt.title(\"Weighted Undirected Network with Node Centrality and Edge Weight\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLwNVaejqZRw","executionInfo":{"status":"aborted","timestamp":1722686057234,"user_tz":-60,"elapsed":6,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["#Draw spring\n","pos = nx.spring_layout(G, k=4)  # k = distância\n","# Draw nodes with size proportional to centrality\n","node_size = [centrality[node] * 2000 for node in G.nodes()]\n","\n","# Draw edges with thickness proportional to weight\n","edge_width = [d[\"weight\"] / 5 for u, v, d in G.edges(data=True)]\n","\n","nx.draw(G, pos, with_labels=True, node_size=node_size, node_color=\"skyblue\", font_size=10, width=edge_width)\n","plt.title(\"Character Network\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gN3nZLWxqb9b","executionInfo":{"status":"aborted","timestamp":1722686057234,"user_tz":-60,"elapsed":5,"user":{"displayName":"Tiago Canário","userId":"08604552682734205379"}}},"outputs":[],"source":["# Draw the network graph with circular layout\n","pos = nx.circular_layout(G)\n","\n","# Draw nodes with size proportional to centrality\n","node_size = [centrality[node] * 2000 for node in G.nodes()]\n","\n","# Draw edges with thickness proportional to weight\n","edge_width = [d[\"weight\"] / 5 for u, v, d in G.edges(data=True)]\n","\n","nx.draw(G, pos, with_labels=True, node_size=node_size, node_color=\"skyblue\", font_size=10, width=edge_width)\n","plt.title(\"Weighted Undirected Network with Node Centrality and Edge Weight (Circular Layout)\")\n","plt.show()"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}